@online{arjovskyInvariantRiskMinimization2020,
  title = {Invariant {{Risk Minimization}}},
  author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
  date = {2020-03-27},
  eprint = {1907.02893},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.02893},
  urldate = {2023-10-19},
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\KLLLN5RK\Arjovsky et al. - 2020 - Invariant Risk Minimization.pdf}
}

@online{arjovskyWassersteinGAN2017,
  title = {Wasserstein {{GAN}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  date = {2017-12-06},
  eprint = {1701.07875},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1701.07875},
  urldate = {2023-09-18},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {GANs,notion},
  file = {C:\Users\victo\Zotero\storage\P2MHJ49X\Arjovsky et al. - 2017 - Wasserstein GAN.pdf}
}

@online{baiRecentAdvancesAdversarial2021,
  title = {Recent {{Advances}} in {{Adversarial Training}} for {{Adversarial Robustness}}},
  author = {Bai, Tao and Luo, Jinqi and Zhao, Jun and Wen, Bihan and Wang, Qian},
  date = {2021-04-20},
  eprint = {2102.01356},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.01356},
  urldate = {2023-09-14},
  abstract = {Adversarial training is one of the most effective approaches to defending deep learning models against adversarial examples. Unlike other defense strategies, adversarial training aims to enhance the robustness of models intrinsically. During the last few years, adversarial training has been studied and discussed from various aspects. A variety of improvements and developments of adversarial training are proposed, which were, however, neglected in existing surveys. For the first time in this survey, we systematically review the recent progress on adversarial training for adversarial robustness with a novel taxonomy. Then we discuss the generalization problems in adversarial training from three perspectives and highlight the challenges which are not fully tackled. Finally, we present potential future directions.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion,REVIEW},
  file = {C:\Users\victo\Zotero\storage\A8YQFZNN\Bai et al. - 2021 - Recent Advances in Adversarial Training for Advers.pdf}
}

@article{belkinReconcilingModernMachine2019,
  title = {Reconciling Modern Machine Learning Practice and the Bias-Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  date = {2019-08-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {116},
  number = {32},
  eprint = {1812.11118},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  pages = {15849--15854},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1903070116},
  url = {http://arxiv.org/abs/1812.11118},
  urldate = {2024-05-01},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\PTYK28C5\Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf}
}

@article{blanchardGeneralizingSeveralRelated,
  title = {Generalizing from {{Several Related Classification Tasks}} to a {{New Unlabeled Sample}}},
  author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
  abstract = {We consider the problem of assigning class labels to an unlabeled test data set, given several labeled training data sets drawn from similar distributions. This problem arises in several applications where data distributions fluctuate because of biological, technical, or other sources of variation. We develop a distributionfree, kernel-based approach to the problem. This approach involves identifying an appropriate reproducing kernel Hilbert space and optimizing a regularized empirical risk over the space. We present generalization error analysis, describe universal kernels, and establish universal consistency of the proposed methodology. Experimental results on flow cytometry data are presented.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\2TUQHJIB\Blanchard et al. - Generalizing from Several Related Classification T.pdf}
}

@book{bovierStatisticalMechanicsDisordered2012,
  title = {Statistical {{Mechanics}} of {{Disordered Systems}}: {{A Mathematical Perspective}}},
  author = {Bovier, Anton},
  date = {2012},
  publisher = {Cambridge University Press}
}

@book{boydConvexOptimization2004,
  title = {Convex {{Optimization}}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  date = {2004},
  publisher = {Cambridge University Press},
  isbn = {978-0-521-83378-3}
}

@inproceedings{brodersenBalancedAccuracyIts2010,
  title = {The {{Balanced Accuracy}} and {{Its Posterior Distribution}}},
  booktitle = {2010 20th {{International Conference}} on {{Pattern Recognition}}},
  author = {Brodersen, Kay Henning and Ong, Cheng Soon and Stephan, Klaas Enno and Buhmann, Joachim M.},
  date = {2010-08},
  pages = {3121--3124},
  publisher = {IEEE},
  location = {Istanbul, Turkey},
  doi = {10.1109/ICPR.2010.764},
  url = {http://ieeexplore.ieee.org/document/5597285/},
  urldate = {2024-02-28},
  abstract = {Evaluating the performance of a classification algorithm critically requires a measure of the degree to which unseen examples have been identified with their correct class labels. In practice, generalizability is frequently estimated by averaging the accuracies obtained on individual crossvalidation folds. This procedure, however, is problematic in two ways. First, it does not allow for the derivation of meaningful confidence intervals. Second, it leads to an optimistic estimate when a biased classifier is tested on an imbalanced dataset. We show that both problems can be overcome by replacing the conventional point estimate of accuracy by an estimate of the posterior distribution of the balanced accuracy.},
  eventtitle = {2010 20th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  isbn = {978-1-4244-7542-1},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\GQ735RM6\Brodersen et al. - 2010 - The Balanced Accuracy and Its Posterior Distributi.pdf}
}

@incollection{buhmannContextSensitiveInformation2011,
  title = {Context {{Sensitive Information}}: {{Model Validation}} by {{Information Theory}}},
  shorttitle = {Context {{Sensitive Information}}},
  booktitle = {Pattern {{Recognition}}},
  author = {Buhmann, Joachim M.},
  editor = {Martínez-Trinidad, José Francisco and Carrasco-Ochoa, Jesús Ariel and Ben-Youssef Brants, Cherif and Hancock, Edwin Robert},
  date = {2011},
  volume = {6718},
  pages = {12--21},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-21587-2_2},
  url = {http://link.springer.com/10.1007/978-3-642-21587-2_2},
  urldate = {2024-04-29},
  abstract = {A theory of patterns analysis has to provide a criterion to filter out the relevant information to identify patterns. The set of potential patterns, also called hypothesis class of the problem, defines admissible explanations of the available data and it specifies the context for a patterns analysis task. Fluctuations in the measurements limit the precision which we can achieve to identify such patterns. Effectively, the distinguishible patterns define a code in a fictitious communication scenario where the selected cost function together with a stochastic data source plays the role of a noisy “channel”. Maximizing the capacity of this channel determines the penalized costs of the pattern analysis problem with a data dependent regularization strength. The tradeoff between informativeness and robustness in statistical inference is mirrored in the balance between high information rate and zero communication error, thereby giving rise to a new notion of context sensitive information.},
  isbn = {978-3-642-21586-5 978-3-642-21587-2},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\PW4KKZCH\Buhmann - 2011 - Context Sensitive Information Model Validation by.pdf}
}

@online{buhmannDataScienceAlgorithms2022,
  title = {Data {{Science Algorithms}} and the {{Rate-Distortion Tradeoff}}},
  author = {Buhmann, Joachim M.},
  date = {2022},
  abstract = {Data Science (DS) algorithms interpret outcomes of empirical experiments with random influences. The input to these algorithms are realizations of random variables and, consequently, DS algorithms necessarily return random variables, even for a deterministic sequence of computations. The uncertainty in the input causes a rate distortion tradeoff in the output when the DS is adapted by learning. We discuss consequences of this setting and present design choices for algorithm validation.},
  pubstate = {prepublished},
  keywords = {notion,posterior agreement},
  file = {C:\Users\victo\Zotero\storage\R3U37HYB\Data Science Algorithms and the Rate-Distortion.pdf}
}

@article{buhmannInformationTheoreticModel,
  title = {Information {{Theoretic Model Selection}} for {{Pattern Analysis}}},
  author = {Buhmann, Joachim M and Chehreghani, Morteza Haghir and Frank, Mario and Streich, Andreas P},
  abstract = {Exploratory data analysis requires (i) to define a set of patterns hypothesized to exist in the data, (ii) to specify a suitable quantification principle or cost function to rank these patterns and (iii) to validate the inferred patterns. For data clustering, the patterns are object partitionings into k groups; for PCA or truncated SVD, the patterns are orthogonal transformations with projections to a low-dimensional space. We propose an information theoretic principle for model selection and model-order selection. Our principle ranks competing pattern cost functions according to their ability to extract context sensitive information from noisy data with respect to the chosen hypothesis class. Sets of approximative solutions serve as a basis for a communication protocol. Analogous to Buhmann (2010), inferred models maximize the so-called approximation capacity that is the mutual information between coarsened training data patterns and coarsened test data patterns. We demonstrate how to apply our validation framework by the well-known Gaussian mixture model and by a multi-label clustering approach for role mining in binary user privilege assignments.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\2T67Q5WK\Buhmann et al. - Information Theoretic Model Selection for Pattern .pdf}
}

@online{buhmannInformationTheoreticModel2010,
  title = {Information Theoretic Model Validation for Clustering},
  author = {Buhmann, Joachim M.},
  date = {2010-06-02},
  eprint = {1006.0375},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1006.0375},
  urldate = {2024-04-29},
  abstract = {Model selection in clustering requires (i) to specify a suitable clustering principle and (ii) to control the model order complexity by choosing an appropriate number of clusters depending on the noise level in the data. We advocate an information theoretic perspective where the uncertainty in the measurements quantizes the set of data partitionings and, thereby, induces uncertainty in the solution space of clusterings. A clustering model, which can tolerate a higher level of fluctuations in the measurements than alternative models, is considered to be superior provided that the clustering solution is equally informative. This tradeoff between informativeness and robustness is used as a model selection criterion. The requirement that data partitionings should generalize from one data set to an equally probable second data set gives rise to a new notion of structure induced information.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\QTIHLVEK\Buhmann - 2010 - Information theoretic model validation for cluster.pdf}
}

@article{buhmannPosteriorAgreementLarge2018,
  title = {Posterior Agreement for Large Parameter-Rich Optimization Problems},
  author = {Buhmann, Joachim M. and Dumazert, Julien and Gronskiy, Alexey and Szpankowski, Wojciech},
  date = {2018-10},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  volume = {745},
  pages = {1--22},
  issn = {03043975},
  doi = {10.1016/j.tcs.2018.04.015},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397518302378},
  urldate = {2023-09-15},
  abstract = {Most real world combinatorial optimization problems are affected by noise in the input data, thus behaving in the high noise limit like large disordered particle systems, e.g. spin glasses or random networks. Due to uncertainty in the input, optimization of such disordered instances should infer stable posterior distributions of solutions conditioned on the noisy input instance. The maximum entropy principle states that the most stable distribution given the noise influence is defined by the Gibbs distribution and it is characterized by the free energy. In this paper, we first provide rigorous asymptotics of the difficult problem to compute the free energy for two combinatorial optimization problems, namely the sparse Minimum Bisection Problem (sMBP) and Lawler’s Quadratic Assignment Problem (LQAP). We prove that both problems exhibit phase transitions equivalent to the discontinuous behavior of Derrida’s Random Energy Model (REM). Furthermore, the derived free energy asymptotics lead to a theoretical justification of a recently introduced concept [3]ofGibbs posterior agreement that measures stability of the Gibbs distributions when the cost function fluctuates due to randomness in the input. This relatively new stability concept may potentially provide a new method to select robust solutions for a large class of optimization problems.},
  langid = {english},
  keywords = {notion,posterior agreement},
  file = {C:\Users\victo\Zotero\storage\URKGLHMD\Buhmann et al. - 2018 - Posterior agreement for large parameter-rich optim.pdf}
}

@online{buhmannPosteriorAgreementModel2022,
  title = {Posterior {{Agreement}} for {{Model Robustness Assessment}} in {{Covariate Shift Scenarios}}},
  author = {Buhmann, Joachim M.},
  date = {2022},
  abstract = {The robustness of algorithms against covariate shifts is a fundamental problem with critical implications for the deployment of machine learning algorithms in the real world. Current evaluation procedures often rely on off-the-shelf metrics such as accuracy scores, that are, however, purpose-built for different ends - the performance assessment - and suffer from a lack of a clear theoretical framework on which they operate. In this paper, we set the desiderata that a robustness metric must possess and we propose a novel principled framework for the robustness assessment problem, that directly follows from the Posterior Agreement (PA) theory, already used as a validation framework in the unsupervised learning setting. Specifically, we extend the PA framework to the covariate shift setting, and we conduct an empirical robustness analysis on two related scenarios: adversarial learning (i.e., evasion attacks), and domain generalization. We illustrate the appropriateness of PA, by evaluating several models under different perturbation levels, distribution shifts, and amounts of affected observations. The results show that the PA score provides a sensible and consistent analysis of the vulnerabilities in learning algorithms even in the presence of few perturbed observations.},
  pubstate = {prepublished},
  keywords = {covariate shift,notion,posterior agreement},
  file = {C:\Users\victo\Zotero\storage\ABKV8XV9\Posterior_Agreement_for_Model_Robustness_Assessment_in_Covariate_Shift_Scenarios.pdf}
}

@inproceedings{caiMultiDomainSentimentClassification2019,
  title = {Multi-{{Domain Sentiment Classification Based}} on {{Domain-Aware Embedding}} and {{Attention}}},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Cai, Yitao and Wan, Xiaojun},
  date = {2019-08},
  pages = {4904--4910},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  location = {Macao, China},
  doi = {10.24963/ijcai.2019/681},
  url = {https://www.ijcai.org/proceedings/2019/681},
  urldate = {2023-09-18},
  abstract = {Sentiment classification is a fundamental task in NLP. However, as revealed by many researches, sentiment classification models are highly domaindependent. It is worth investigating to leverage data from different domains to improve the classification performance in each domain. In this work, we propose a novel completely-shared multidomain neural sentiment classification model to learn domain-aware word embeddings and make use of domain-aware attention mechanism. Our model first utilizes BiLSTM for domain classification and extracts domain-specific features for words, which are then combined with general word embeddings to form domain-aware word embeddings. Domain-aware word embeddings are fed into another BiLSTM to extract sentence features. The domain-aware attention mechanism is used for selecting significant features, by using the domainaware sentence representation as the query vector. Evaluation results on public datasets with 16 different domains demonstrate the efficacy of our proposed model. Further experiments show the generalization ability and the transferability of our model.},
  eventtitle = {Twenty-{{Eighth International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-19}}\vphantom\{\}},
  isbn = {978-0-9992411-4-1},
  langid = {english},
  keywords = {multi-domain embedding,notion,sentiment classification},
  file = {C:\Users\victo\Zotero\storage\FG3MZNWG\Cai i Wan - 2019 - Multi-Domain Sentiment Classification Based on Dom.pdf}
}

@online{carliniEvaluatingRobustnessNeural2017,
  title = {Towards {{Evaluating}} the {{Robustness}} of {{Neural Networks}}},
  author = {Carlini, Nicholas and Wagner, David},
  date = {2017-03-22},
  eprint = {1608.04644},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1608.04644},
  urldate = {2024-05-07},
  abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to find adversarial examples from 95\% to 0.5\%.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security},
  file = {C:\Users\victo\Zotero\storage\4UUHZGDU\Carlini i Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf}
}

@book{casellaStatisticalInference2002,
  title = {Statistical {{Inference}}},
  author = {Casella, George and L. Berger, Roger},
  date = {2002},
  edition = {Second},
  publisher = {Wadsworth Group Duxbury}
}

@book{logicofscience,
  title = {Probability Theory: The Logic of Science},
  author = {Jaynes, E. T.},
  date = {2003},
  edition = {First},
  publisher = {Cambridge University Press},
}

@article{chehreghaniInformationTheoreticModel,
  title = {Information {{Theoretic Model Validation}} for {{Spectral Clustering}}},
  author = {Chehreghani, Morteza Haghir and Busetto, Alberto Giovanni and Buhmann, Joachim M},
  abstract = {Model validation constitutes a fundamental step in data clustering. The central question is: Which cluster model and how many clusters are most appropriate for a certain application? In this study, we introduce a method for the validation of spectral clustering based upon approximation set coding. In particular, we compare correlation and pairwise clustering to analyze the correlations of temporal gene expression profiles. To evaluate and select clustering models, we calculate their reliable informativeness. Experimental results in the context of gene expression analysis show that pairwise clustering yields superior amounts of reliable information. The analysis results are consistent with the Bayesian Information Criterion (BIC), and exhibit higher generality than BIC.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\NAMZ65IM\Chehreghani et al. - Information Theoretic Model Validation for Spectra.pdf}
}

@online{cohenCertifiedAdversarialRobustness2019,
  title = {Certified {{Adversarial Robustness}} via {{Randomized Smoothing}}},
  author = {Cohen, Jeremy M. and Rosenfeld, Elan and Kolter, J. Zico},
  date = {2019-06-15},
  eprint = {1902.02918},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.02918},
  urldate = {2023-09-15},
  abstract = {We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the 2 norm. This “randomized smoothing” technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in 2 norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49\% under adversarial perturbations with 2 norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified 2 robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at http: //github.com/locuslab/smoothing.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\ESKWP59X\Cohen et al. - 2019 - Certified Adversarial Robustness via Randomized Sm.pdf}
}

@online{croceRobustBenchStandardizedAdversarial2021a,
  title = {{{RobustBench}}: A Standardized Adversarial Robustness Benchmark},
  shorttitle = {{{RobustBench}}},
  author = {Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and Debenedetti, Edoardo and Flammarion, Nicolas and Chiang, Mung and Mittal, Prateek and Hein, Matthias},
  date = {2021-10-31},
  eprint = {2010.09670},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2010.09670},
  urldate = {2023-10-19},
  abstract = {As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models. We evaluate adversarial robustness with AutoAttack [28], an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks [142], especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at https://robustbench.github.io/, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in ∞- and 2-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library https://github.com/RobustBench/robustbench that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\ASCFLU7N\Croce et al. - 2021 - RobustBench a standardized adversarial robustness.pdf}
}

@online{dasKeepingBadGuys2017,
  title = {Keeping the {{Bad Guys Out}}: {{Protecting}} and {{Vaccinating Deep Learning}} with {{JPEG Compression}}},
  shorttitle = {Keeping the {{Bad Guys Out}}},
  author = {Das, Nilaksh and Shanbhogue, Madhuri and Chen, Shang-Tse and Hohman, Fred and Chen, Li and Kounavis, Michael E. and Chau, Duen Horng},
  date = {2017-05-08},
  eprint = {1705.02900},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1705.02900},
  urldate = {2024-07-31},
  abstract = {Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security},
  file = {C:\Users\victo\Zotero\storage\4F9JIYPS\Das et al. - 2017 - Keeping the Bad Guys Out Protecting and Vaccinati.pdf}
}

@misc{lecun1998mnist,
  title = {{The MNIST database of handwritten digits}},
  author = {Yann LeCun and Corinna Cortes and Christopher J.C. Burges},
  year = {1998},
  howpublished = {\url{http://yann.lecun.com/exdb/mnist}},
  note = {Accessed: 2024-09-07}
}

@online{euligDiagViB6DiagnosticBenchmark2021,
  title = {{{DiagViB-6}}: {{A Diagnostic Benchmark Suite}} for {{Vision Models}} in the {{Presence}} of {{Shortcut}} and {{Generalization Opportunities}}},
  shorttitle = {{{DiagViB-6}}},
  author = {Eulig, Elias and Saranrittichai, Piyapat and Mummadi, Chaithanya Kumar and Rambach, Kilian and Beluch, William and Shi, Xiahan and Fischer, Volker},
  date = {2021-10-08},
  eprint = {2108.05779},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2108.05779},
  urldate = {2023-10-19},
  abstract = {Common deep neural networks (DNNs) for image classification have been shown to rely on shortcut opportunities (SO) in the form of predictive and easy-to-represent visual factors. This is known as shortcut learning and leads to impaired generalization. In this work, we show that common DNNs also suffer from shortcut learning when predicting only basic visual object factors of variation (FoV) such as shape, color, or texture. We argue that besides shortcut opportunities, generalization opportunities (GO) are also an inherent part of real-world vision data and arise from partial independence between predicted classes and FoVs. We also argue that it is necessary for DNNs to exploit GO to overcome shortcut learning. Our core contribution is to introduce the Diagnostic Vision Benchmark suite DiagViB-6, which includes datasets and metrics to study a network’s shortcut vulnerability and generalization capability for six independent FoV. In particular, DiagViB-6 allows controlling the type and degree of SO and GO in a dataset. We benchmark a wide range of popular vision architectures and show that they can exploit GO only to a limited extent.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\victo\Zotero\storage\3KZNW5YG\Eulig et al. - 2021 - DiagViB-6 A Diagnostic Benchmark Suite for Vision.pdf}
}

@online{ganinDomainAdversarialTrainingNeural2016,
  title = {Domain-{{Adversarial Training}} of {{Neural Networks}}},
  author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and Marchand, Mario and Lempitsky, Victor},
  date = {2016-05-26},
  eprint = {1505.07818},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1505.07818},
  urldate = {2023-09-18},
  abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {domain-adversarial training,notion},
  file = {C:\Users\victo\Zotero\storage\W5GGV367\Ganin et al. - 2016 - Domain-Adversarial Training of Neural Networks.pdf}
}

@article{geirhosShortcutLearningDeep2020,
  title = {Shortcut {{Learning}} in {{Deep Neural Networks}}},
  author = {Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
  date = {2020-11-10},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {2},
  number = {11},
  eprint = {2004.07780},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio},
  pages = {665--673},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00257-z},
  url = {http://arxiv.org/abs/2004.07780},
  urldate = {2024-06-05},
  abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today’s machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distill how many of deep learning’s problems can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {C:\Users\victo\Zotero\storage\HYHMQXC6\Geirhos et al. - 2020 - Shortcut Learning in Deep Neural Networks.pdf}
}

@online{goodfellowExplainingHarnessingAdversarial2015,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  date = {2015-03-20},
  eprint = {1412.6572},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1412.6572},
  urldate = {2023-09-18},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {adversarial perturbations,FGSM,notion},
  file = {C:\Users\victo\Zotero\storage\P5UQXMKG\Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf}
}

@software{GoogleRobustnessMetrics,
  title = {Google {{Robustness Metrics}}},
  url = {https://github.com/google-research/robustness_metrics}
}

@inproceedings{gronskiyFreeEnergyAsymptotics2018,
  title = {Free {{Energy Asymptotics}} for {{Problems}} with {{Weak Solution Dependencies}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{Information Theory}} ({{ISIT}})},
  author = {Gronskiy, Alexey and Buhmann, Joachim M. and Szpankowski, Wojciech},
  date = {2018-06},
  pages = {2132--2136},
  publisher = {IEEE},
  location = {Vail, CO},
  doi = {10.1109/ISIT.2018.8437718},
  url = {https://ieeexplore.ieee.org/document/8437718/},
  urldate = {2023-09-15},
  abstract = {Information theoretic properties of large combinatorial systems enable us in better understanding their solution structure and provide insights how to optimize them in a robust manner. In this paper, we revisit the idea of characterizing structural information in solutions for combinatorial problems by information theoretic properties. We provide theorems on analytic expressions of the asymptotic free energy and entropy of solutions with weak dependencies for strongly disordered combinatorial optimization problems, e.g. the sparse Minimum Bisection Problem (sMBP). We prove that the free energy of sMBP with random edge weights exhibits phase transitions equivalent to Derrida’s Random Energy Model (REM). Specifically, we analyze the dependency structure between two arbitrary solutions and prove that the influence of correlations on the free energy and the relative entropy vanishes.},
  eventtitle = {2018 {{IEEE International Symposium}} on {{Information Theory}} ({{ISIT}})},
  isbn = {978-1-5386-4781-3},
  langid = {english},
  keywords = {information theory,notion,sparse minimum bisection problem},
  file = {C:\Users\victo\Zotero\storage\7SX4C6QE\Gronskiy et al. - 2018 - Free Energy Asymptotics for Problems with Weak Sol.pdf}
}

@book{jolliffe2002principal,
  title={Principal Component Analysis},
  author={Jolliffe, Ian T.},
  year={2002},
  publisher={Springer},
  address={New York},
  edition={2nd},
  series={Springer Series in Statistics},
  doi={10.1007/b98835},
}

@article{grunwaldMinimumDescriptionLength2019,
  title = {Minimum {{Description Length Revisited}}},
  author = {Grünwald, Peter and Roos, Teemu},
  date = {2019-12},
  journaltitle = {International Journal of Mathematics for Industry},
  shortjournal = {Int. J. Math. Ind.},
  volume = {11},
  number = {01},
  eprint = {1908.08484},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  pages = {1930001},
  issn = {2661-3352, 2661-3344},
  doi = {10.1142/S2661335219300018},
  url = {http://arxiv.org/abs/1908.08484},
  urldate = {2024-02-14},
  abstract = {This is an up-to-date introduction to and overview of the Minimum Description Length (MDL) Principle, a theory of inductive inference that can be applied to general problems in statistics, machine learning and pattern recognition. While MDL was originally based on data compression ideas, this introduction can be read without any knowledge thereof. It takes into account all major developments since 2007, the last time an extensive overview was written. These include new methods for model selection and averaging and hypothesis testing, as well as the first completely general definition of MDL estimators. Incorporating these developments, MDL can be seen as a powerful extension of both penalized likelihood and Bayesian approaches, in which penalization functions and prior distributions are replaced by more general luckiness functions, average-case methodology is replaced by a more robust worst-case approach, and in which methods classically viewed as highly distinct, such as AIC vs BIC and cross-validation vs Bayes can, to a large extent, be viewed from a unified perspective.},
  langid = {english},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,notion,Statistics - Machine Learning,Statistics - Methodology},
  file = {C:\Users\victo\Zotero\storage\G5363X5I\Grünwald i Roos - 2019 - Minimum Description Length Revisited.pdf}
}

@article{guoComprehensiveEvaluationFramework2023,
  title = {A Comprehensive Evaluation Framework for Deep Model Robustness},
  author = {Guo, Jun and Bao, Wei and Wang, Jiakai and Ma, Yuqing and Gao, Xinghai and Xiao, Gang and Liu, Aishan and Dong, Jian and Liu, Xianglong and Wu, Wenjun},
  date = {2023-05},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {137},
  pages = {109308},
  issn = {00313203},
  doi = {10.1016/j.patcog.2023.109308},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320323000092},
  urldate = {2024-04-30},
  langid = {english},
  file = {C:\Users\victo\Zotero\storage\6ZE8GI7H\Guo et al. - 2023 - A comprehensive evaluation framework for deep mode.pdf}
}

@book{gutIntermediateCourseProbability2009,
  title = {An {{Intermediate Course}} on {{Probability}}},
  author = {Gut, Allan},
  date = {2009},
  edition = {Second},
  publisher = {Springer}
}

@online{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  eprint = {2006.11239},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2023-10-24},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {DDPM,notion},
  file = {C:\Users\victo\Zotero\storage\3B9LVTHW\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@online{huDomainInvariantFeatureDistillation2019,
  title = {Domain-{{Invariant Feature Distillation}} for {{Cross-Domain Sentiment Classification}}},
  author = {Hu, Mengting and Wu, Yike and Zhao, Shiwan and Guo, Honglei and Cheng, Renhong and Su, Zhong},
  date = {2019-08-24},
  eprint = {1908.09122},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1908.09122},
  urldate = {2023-09-18},
  abstract = {Cross-domain sentiment classification has drawn much attention in recent years. Most existing approaches focus on learning domaininvariant representations in both the source and target domains, while few of them pay attention to the domain-specific information. Despite the non-transferability of the domainspecific information, simultaneously learning domain-dependent representations can facilitate the learning of domain-invariant representations. In this paper, we focus on aspectlevel cross-domain sentiment classification, and propose to distill the domain-invariant sentiment features with the help of an orthogonal domain-dependent task, i.e. aspect detection, which is built on the aspects varying widely in different domains. We conduct extensive experiments on three public datasets and the experimental results demonstrate the effectiveness of our method.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {domain-invariant feature distillation,notion,sentiment classification},
  file = {C:\Users\victo\Zotero\storage\Q2TYR5SH\Hu et al. - 2019 - Domain-Invariant Feature Distillation for Cross-Do.pdf}
}

@online{ilyasAdversarialExamplesAre2019,
  title = {Adversarial {{Examples Are Not Bugs}}, {{They Are Features}}},
  author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  date = {2019-08-12},
  eprint = {1905.02175},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.02175},
  urldate = {2024-05-02},
  abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\W528CVSA\Ilyas et al. - 2019 - Adversarial Examples Are Not Bugs, They Are Featur.pdf}
}

@article{ishidaWeNeedZero2020,
  title = {Do {{We Need Zero Training Loss After Achieving Zero Training Error}}?},
  author = {Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
  date = {2020},
  abstract = {Overparameterized deep networks have the capacity to memorize training data with zero training error. Even after memorization, the training loss continues to approach zero, making the model overconfident and the test performance degraded. Since existing regularizers do not directly aim to avoid zero training loss, it is hard to tune their hyperparameters in order to maintain a fixed/preset level of training loss. We propose a direct solution called flooding that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the flood level. Our approach makes the loss float around the flood level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the flood level. This can be implemented with one line of code and is compatible with any stochastic optimizer and other regularizers. With flooding, the model will continue to “random walk” with the same non-zero training loss, and we expect it to drift into an area with a flat loss landscape that leads to better generalization. We experimentally show that flooding improves performance and, as a byproduct, induces a double descent curve of the test loss.},
  langid = {english},
  keywords = {flooding,notion,regularization},
  file = {C:\Users\victo\Zotero\storage\9NM7QNVK\Ishida et al. - Do We Need Zero Training Loss After Achieving Zero.pdf}
}

@article{jimenezInductiveBiasDeep,
  title = {The Inductive Bias of Deep Learning: {{Connecting}} Weights and Functions},
  author = {Jimenez, Ortiz},
  langid = {english},
  file = {C:\Users\victo\Zotero\storage\J432TJPE\Jimenez - The inductive bias of deep learning Connecting we.pdf}
}

@online{jinBERTReallyRobust2020,
  title = {Is {{BERT Really Robust}}? {{A Strong Baseline}} for {{Natural Language Attack}} on {{Text Classification}} and {{Entailment}}},
  shorttitle = {Is {{BERT Really Robust}}?},
  author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  date = {2020-04-08},
  eprint = {1907.11932},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1907.11932},
  urldate = {2023-09-18},
  abstract = {Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate natural adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate the advantages of this framework in three ways: (1) effective---it outperforms state-of-the-art attacks in terms of success rate and perturbation rate, (2) utility-preserving---it preserves semantic content and grammaticality, and remains correctly classified by humans, and (3) efficient---it generates adversarial text with computational complexity linear to the text length. *The code, pre-trained target models, and test examples are available at https://github.com/jind11/TextFooler.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {adversarial attack,notion},
  file = {C:\Users\victo\Zotero\storage\4XM6VJ5Z\Jin et al. - 2020 - Is BERT Really Robust A Strong Baseline for Natur.pdf}
}

@incollection{khoslaUndoingDamageDataset2012,
  title = {Undoing the {{Damage}} of {{Dataset Bias}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2012},
  author = {Khosla, Aditya and Zhou, Tinghui and Malisiewicz, Tomasz and Efros, Alexei A. and Torralba, Antonio},
  editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  editora = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editoratype = {redactor},
  date = {2012},
  volume = {7572},
  pages = {158--171},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-33718-5_12},
  url = {http://link.springer.com/10.1007/978-3-642-33718-5_12},
  urldate = {2024-05-07},
  isbn = {978-3-642-33717-8 978-3-642-33718-5},
  langid = {english},
  file = {C:\Users\victo\Zotero\storage\WV6Y5PB3\Khosla et al. - 2012 - Undoing the Damage of Dataset Bias.pdf}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  eprint = {1412.6980},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2024-06-04},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\9MAP8ZRF\Kingma i Ba - 2017 - Adam A Method for Stochastic Optimization.pdf}
}

@online{kohWILDSBenchmarkIntheWild2021,
  title = {{{WILDS}}: {{A Benchmark}} of in-the-{{Wild Distribution Shifts}}},
  shorttitle = {{{WILDS}}},
  author = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and Lee, Tony and David, Etienne and Stavness, Ian and Guo, Wei and Earnshaw, Berton A. and Haque, Imran S. and Beery, Sara and Leskovec, Jure and Kundaje, Anshul and Pierson, Emma and Levine, Sergey and Finn, Chelsea and Liang, Percy},
  date = {2021-07-16},
  eprint = {2012.07421},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.07421},
  urldate = {2024-03-28},
  abstract = {Distribution shifts—where the training distribution differs from the test distribution—can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present Wilds, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. Code and leaderboards are available at https://wilds.stanford.edu.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\7DUD3KC8\Koh et al. - 2021 - WILDS A Benchmark of in-the-Wild Distribution Shi.pdf}
}

@article{krizhevskyLearningMultipleLayers,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  langid = {english},
  file = {C:\Users\victo\Zotero\storage\IQ9LAIT2\Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf}
}

@online{liangComprehensiveSurveyTestTime2023,
  title = {A {{Comprehensive Survey}} on {{Test-Time Adaptation}} under {{Distribution Shifts}}},
  author = {Liang, Jian and He, Ran and Tan, Tieniu},
  date = {2023-03-27},
  eprint = {2303.15361},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.15361},
  urldate = {2024-04-13},
  abstract = {Machine learning methods strive to acquire a robust model during training that can generalize well to test samples, even under distribution shifts. However, these methods often suffer from a performance drop due to unknown test distributions. Test-time adaptation (TTA), an emerging paradigm, has the potential to adapt a pre-trained model to unlabeled data during testing, before making predictions. Recent progress in this paradigm highlights the significant benefits of utilizing unlabeled data for training self-adapted models prior to inference. In this survey, we divide TTA into several distinct categories, namely, test-time (source-free) domain adaptation, test-time batch adaptation, online test-time adaptation, and test-time prior adaptation. For each category, we provide a comprehensive taxonomy of advanced algorithms, followed by a discussion of different learning scenarios. Furthermore, we analyze relevant applications of TTA and discuss open challenges and promising areas for future research. A comprehensive list of TTA methods can be found at https://github.com/tim-learn/awesome-test-time-adaptation.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {C:\Users\victo\Zotero\storage\NIZNAR94\Liang et al. - 2023 - A Comprehensive Survey on Test-Time Adaptation und.pdf}
}

@article{liLearningGeneralizeMetaLearning2018,
  title = {Learning to {{Generalize}}: {{Meta-Learning}} for {{Domain Generalization}}},
  shorttitle = {Learning to {{Generalize}}},
  author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy},
  date = {2018-04-29},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11596},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11596},
  urldate = {2024-05-06},
  abstract = {Domain shift refers to the well known problem that a model trained in one source domain performs poorly when applied to a target domain with different statistics. Domain Generalization (DG) techniques attempt to alleviate this issue by producing models which by design generalize well to novel testing domains. We propose a novel meta-learning method for domain generalization. Rather than designing a specific model that is robust to domain shift as in most previous DG work, we propose a model agnostic training procedure for DG. Our algorithm simulates train/test domain shift during training by synthesizing virtual testing domains within each mini-batch. The meta-optimization objective requires that steps to improve training domain performance should also improve testing domain performance. This meta-learning procedure trains models with good generalization ability to novel domains. We evaluate our method and achieve state of the art results on a recent cross-domain image classification benchmark, as well demonstrating its potential on two classic reinforcement learning tasks.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\QZKA9RZ4\Li et al. - 2018 - Learning to Generalize Meta-Learning for Domain G.pdf}
}

@article{liReviewAdversarialAttack2022,
  title = {A {{Review}} of {{Adversarial Attack}} and {{Defense}} for {{Classification Methods}}},
  author = {Li, Yao and Cheng, Minhao and Hsieh, Cho-Jui and Lee, Thomas C. M.},
  date = {2022-10-02},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {76},
  number = {4},
  eprint = {2111.09961},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {329--345},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2021.2006781},
  url = {http://arxiv.org/abs/2111.09961},
  urldate = {2024-05-07},
  abstract = {Despite the efficiency and scalability of machine learning systems, recent studies have demonstrated that many classification methods, especially deep neural networks (DNNs), are vulnerable to adversarial examples; i.e., examples that are carefully crafted to fool a well-trained classification model while being indistinguishable from natural data to human. This makes it potentially unsafe to apply DNNs or related methods in security-critical areas. Since this issue was first identified by Biggio et al. (2013) and Szegedy et al. (2014), much work has been done in this field, including the development of attack methods to generate adversarial examples and the construction of defense techniques to guard against such examples. This paper aims to introduce this topic and its latest developments to the statistical community, primarily focusing on the generation and guarding of adversarial examples. Computing codes (in python and R) used in the numerical experiments are publicly available for readers to explore the surveyed methods. It is the hope of the authors that this paper will encourage more statisticians to work on this important and exciting field of generating and defending against adversarial examples.},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\TYIAWUFT\Li et al. - 2022 - A Review of Adversarial Attack and Defense for Cla.pdf}
}

@article{liTokenAwareVirtualAdversarial2021,
  title = {Token-{{Aware Virtual Adversarial Training}} in {{Natural Language Understanding}}},
  author = {Li, Linyang and Qiu, Xipeng},
  date = {2021-05-18},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {35},
  number = {9},
  pages = {8410--8418},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v35i9.17022},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17022},
  urldate = {2023-09-17},
  abstract = {Gradient-based adversarial training is widely used in improving the robustness of neural networks, while it cannot be easily adapted to natural language processing tasks since the embedding space is discrete. In natural language processing fields, virtual adversarial training is introduced since texts are discrete and cannot be perturbed by gradients directly. Alternatively, virtual adversarial training, which generates perturbations on the embedding space, is introduced in NLP tasks. Despite its success, existing virtual adversarial training methods generate perturbations roughly constrained by Frobenius normalization balls. To craft fine-grained perturbations, we propose a Token-Aware Virtual Adversarial Training method. We introduce a token-level accumulated perturbation vocabulary to initialize the perturbations better and use a tokenlevel normalization ball to constrain these perturbations pertinently. Experiments show that our method improves the performance of pre-trained models such as BERT and ALBERT in various tasks by a considerable margin. The proposed method improves the score of the GLUE benchmark from 78.3 to 80.9 using BERT model and it also enhances the performance of sequence labeling and text classification tasks.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\PH3C4GH3\Li i Qiu - 2021 - Token-Aware Virtual Adversarial Training in Natura.pdf}
}

@online{liTransferableEndtoEndAspectbased2019,
  title = {Transferable {{End-to-End Aspect-based Sentiment Analysis}} with {{Selective Adversarial Learning}}},
  author = {Li, Zheng and Li, Xin and Wei, Ying and Bing, Lidong and Zhang, Yu and Yang, Qiang},
  date = {2019-10-30},
  eprint = {1910.14192},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1910.14192},
  urldate = {2023-09-18},
  abstract = {Joint extraction of aspects and sentiments can be effectively formulated as a sequence labeling problem. However, such formulation hinders the effectiveness of supervised methods due to the lack of annotated sequence data in many domains. To address this issue, we firstly explore an unsupervised domain adaptation setting for this task. Prior work can only use common syntactic relations between aspect and opinion words to bridge the domain gaps, which highly relies on external linguistic resources. To resolve it, we propose a novel Selective Adversarial Learning (SAL) method to align the inferred correlation vectors that automatically capture their latent relations. The SAL method can dynamically learn an alignment weight for each word such that more important words can possess higher alignment weights to achieve finegrained (word-level) adaptation. Empirically, extensive experiments1 demonstrate the effectiveness of the proposed SAL method.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion,selective adversarial learning,sentiment classification},
  file = {C:\Users\victo\Zotero\storage\ZDJ7SGSI\Li et al. - 2019 - Transferable End-to-End Aspect-based Sentiment Ana.pdf}
}

@inproceedings{liuAdversarialMultitaskLearning2017a,
  title = {Adversarial {{Multi-task Learning}} for {{Text Classification}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for           {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
  date = {2017},
  pages = {1--10},
  publisher = {Association for Computational Linguistics},
  location = {Vancouver, Canada},
  doi = {10.18653/v1/P17-1001},
  url = {http://aclweb.org/anthology/P17-1001},
  urldate = {2023-10-11},
  abstract = {Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks.},
  eventtitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for           {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  keywords = {domain-adversarial training,multi-task learning,notion},
  file = {C:\Users\victo\Zotero\storage\EPX4RZ3E\Liu et al. - 2017 - Adversarial Multi-task Learning for Text Classific.pdf}
}

@inproceedings{liuFloodingXImprovingBERT2022,
  title = {Flooding-{{X}}: {{Improving BERT}}’s {{Resistance}} to {{Adversarial Attacks}} via {{Loss-Restricted Fine-Tuning}}},
  shorttitle = {Flooding-{{X}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Qin and Zheng, Rui and Rong, Bao and Liu, Jingyi and Liu, ZhiHua and Cheng, Zhanzhan and Qiao, Liang and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  date = {2022},
  pages = {5634--5644},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.386},
  url = {https://aclanthology.org/2022.acl-long.386},
  urldate = {2023-09-14},
  abstract = {Adversarial robustness has attracted much attention recently, and the mainstream solution is adversarial training. However, the tradition of generating adversarial perturbations for each input embedding (in the settings of NLP) scales up the training computational complexity by the number of gradient steps it takes to obtain the adversarial samples. To address this problem, we leverage Flooding method which primarily aims at better generalization and we find promising in defending adversarial attacks. We further propose an effective criterion to bring hyper-parameter-dependent flooding into effect with a narrowed-down search space by measuring how the gradient steps taken within one epoch affect the loss of each batch. Our approach requires zero adversarial sample for training, and its time consumption is equivalent to fine-tuning, which can be 2-15 times faster than standard adversarial training. We experimentally show that our method improves BERT’s resistance to textual adversarial attacks by a large margin, and achieves state-of-the-art robust accuracy on various text classification and GLUE tasks.},
  eventtitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  keywords = {flooding,notion},
  file = {C:\Users\victo\Zotero\storage\L8KLM8KM\Liu et al. - 2022 - Flooding-X Improving BERT’s Resistance to Adversa.pdf}
}

@online{liuOutOfDistributionGeneralizationSurvey2023,
  title = {Towards {{Out-Of-Distribution Generalization}}: {{A Survey}}},
  shorttitle = {Towards {{Out-Of-Distribution Generalization}}},
  author = {Liu, Jiashuo and Shen, Zheyan and He, Yue and Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Cui, Peng},
  date = {2023-07-27},
  eprint = {2108.13624},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2108.13624},
  urldate = {2024-05-06},
  abstract = {Traditional machine learning paradigms are based on the assumption that both training and test data follow the same statistical pattern, which is mathematically referred to as Independent and Identically Distributed (i.i.d.). However, in real-world applications, this i.i.d. assumption often fails to hold due to unforeseen distributional shifts, leading to considerable degradation in model performance upon deployment. This observed discrepancy indicates the significance of investigating the Out-of-Distribution (OOD) generalization problem. OOD generalization is an emerging topic of machine learning research that focuses on complex scenarios wherein the distributions of the test data differ from those of the training data. This paper represents the first comprehensive, systematic review of OOD generalization, encompassing a spectrum of aspects from problem definition, methodological development, and evaluation procedures, to the implications and future directions of the field. Our discussion begins with a precise, formal characterization of the OOD generalization problem. Following that, we categorize existing methodologies into three segments: unsupervised representation learning, supervised model learning, and optimization, according to their positions within the overarching learning process. We provide an in-depth discussion on representative methodologies for each category, further elucidating the theoretical links between them. Subsequently, we outline the prevailing benchmark datasets employed in OOD generalization studies. To conclude, we overview the existing body of work in this domain and suggest potential avenues for future research on OOD generalization. A summary of the OOD generalization methodologies surveyed in this paper can be accessed at http://out-of-distribution-generalization.com.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion},
  file = {C:\Users\victo\Zotero\storage\6N3QK43K\Liu et al. - 2023 - Towards Out-Of-Distribution Generalization A Surv.pdf}
}

@book{m.bishopPatternRecognitionMachine2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {M. Bishop, Cristopher},
  date = {2006},
  publisher = {Springer},
  location = {Cambridge, Massachusetts}
}

@online{madryDeepLearningModels2019,
  title = {Towards {{Deep Learning Models Resistant}} to {{Adversarial Attacks}}},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  date = {2019-09-04},
  eprint = {1706.06083},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.06083},
  urldate = {2023-09-14},
  abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion,PGD},
  file = {C:\Users\victo\Zotero\storage\DIAHJT5X\Madry et al. - 2019 - Towards Deep Learning Models Resistant to Adversar.pdf}
}

@online{miyatoVirtualAdversarialTraining2018,
  title = {Virtual {{Adversarial Training}}: {{A Regularization Method}} for {{Supervised}} and {{Semi-Supervised Learning}}},
  shorttitle = {Virtual {{Adversarial Training}}},
  author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
  date = {2018-06-27},
  eprint = {1704.03976},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1704.03976},
  urldate = {2023-09-18},
  abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is defined as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method defines the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only “virtually” adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\RHKNNYW8\Miyato et al. - 2018 - Virtual Adversarial Training A Regularization Met.pdf}
}

@inproceedings{morrisTextAttackFrameworkAdversarial2020,
  title = {{{TextAttack}}: {{A Framework}} for {{Adversarial Attacks}}, {{Data Augmentation}}, and {{Adversarial Training}} in {{NLP}}},
  shorttitle = {{{TextAttack}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  date = {2020},
  pages = {119--126},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.emnlp-demos.16},
  url = {https://www.aclweb.org/anthology/2020.emnlp-demos.16},
  urldate = {2023-09-18},
  abstract = {While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack’s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.},
  eventtitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  langid = {english},
  keywords = {adversarial attack,notion},
  file = {C:\Users\victo\Zotero\storage\6WRCS93R\Morris et al. - 2020 - TextAttack A Framework for Adversarial Attacks, D.pdf}
}

@online{muandetDomainGeneralizationInvariant2013,
  title = {Domain {{Generalization}} via {{Invariant Feature Representation}}},
  author = {Muandet, Krikamol and Balduzzi, David and Schölkopf, Bernhard},
  date = {2013-01-10},
  eprint = {1301.2115},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1301.2115},
  urldate = {2024-05-06},
  abstract = {This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\F9TJZQL3\Muandet et al. - 2013 - Domain Generalization via Invariant Feature Repres.pdf}
}

@book{n.vapnikNatureStatisticalLearning2000,
  title = {The {{Nature}} of {{Statistical Learning Theory}}},
  author = {N. Vapnik, Vladimir},
  date = {2000},
  edition = {Second}
}

@online{northcuttPervasiveLabelErrors2021,
  title = {Pervasive {{Label Errors}} in {{Test Sets Destabilize Machine Learning Benchmarks}}},
  author = {Northcutt, Curtis G. and Athalye, Anish and Mueller, Jonas},
  date = {2021-11-07},
  eprint = {2103.14749},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2103.14749},
  urldate = {2024-03-29},
  abstract = {We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of at least 3.3\% errors across the 10 datasets, where for example label errors comprise at least 6\% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (51\% of the algorithmically-flagged candidates are indeed erroneously labeled, on average across the datasets). Traditionally, machine learning practitioners choose which model to deploy based on test accuracy — our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets. Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6\%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5\%. Test set errors across the 10 datasets can be viewed at https://labelerrors.com and all label errors can be reproduced by https://github.com/cleanlab/label-errors.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\QKMMS8LR\Northcutt et al. - 2021 - Pervasive Label Errors in Test Sets Destabilize Ma.pdf}
}

@book{p.murphyProbabilisticMachineLearning2022,
  title = {Probabilistic {{Machine Learning}}. {{An Introduction}}.},
  author = {P. Murphy, Kevin},
  date = {2022},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  isbn = {978-0-262-04682-4}
}

@article{peiMultiAdversarialDomainAdaptation,
  title = {Multi-{{Adversarial Domain Adaptation}}},
  author = {Pei, Zhongyi and Cao, Zhangjie and Long, Mingsheng and Wang, Jianmin},
  abstract = {Recent advances in deep domain adaptation reveal that adversarial learning can be embedded into deep networks to learn transferable features that reduce distribution discrepancy between the source and target domains. Existing domain adversarial adaptation methods based on single domain discriminator only align the source and target data distributions without exploiting the complex multimode structures. In this paper, we present a multi-adversarial domain adaptation (MADA) approach, which captures multimode structures to enable fine-grained alignment of different data distributions based on multiple domain discriminators. The adaptation can be achieved by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Empirical evidence demonstrates that the proposed model outperforms state of the art methods on standard domain adaptation datasets.},
  langid = {english},
  keywords = {domain-adversarial training,notion},
  file = {C:\Users\victo\Zotero\storage\65PT958P\Pei et al. - Multi-Adversarial Domain Adaptation.pdf}
}

@online{pintorFastMinimumnormAdversarial2021,
  title = {Fast {{Minimum-norm Adversarial Attacks}} through {{Adaptive Norm Constraints}}},
  author = {Pintor, Maura and Roli, Fabio and Brendel, Wieland and Biggio, Battista},
  date = {2021-11-19},
  eprint = {2102.12827},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.12827},
  urldate = {2024-05-01},
  abstract = {Evaluating adversarial robustness amounts to finding the minimum perturbation needed to have an input sample misclassified. The inherent complexity of the underlying optimization requires current gradient-based attacks to be carefully tuned, initialized, and possibly executed for many computationally-demanding iterations, even if specialized to a given perturbation model. In this work, we overcome these limitations by proposing a fast minimum-norm (FMN) attack that works with different p-norm perturbation models (p = 0, 1, 2, ∞), is robust to hyperparameter choices, does not require adversarial starting points, and converges within few lightweight steps. It works by iteratively finding the sample misclassified with maximum confidence within an p-norm constraint of size , while adapting to minimize the distance of the current sample to the decision boundary. Extensive experiments show that FMN significantly outperforms existing 0, 1, and ∞-norm attacks in terms of perturbation size, convergence speed and computation time, while reporting comparable performances with state-of-the-art 2-norm attacks. Our open-source code is available at: https://github.com/pralab/Fast-Minimum-Norm-FMN-Attack.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\IXE7KHSP\Pintor et al. - 2021 - Fast Minimum-norm Adversarial Attacks through Adap.pdf}
}

@book{quinonero-candelaDatasetShiftMachine2009,
  title = {Dataset Shift in Machine Learning},
  editor = {Quiñonero-Candela, Joaquin},
  date = {2009},
  series = {Neural Information Processing Series},
  publisher = {MIT Press},
  location = {Cambridge, Mass},
  isbn = {978-0-262-17005-5 978-0-262-54587-7},
  langid = {english},
  pagetotal = {229},
  keywords = {Machine learning},
  annotation = {OCLC: ocn227205909},
  file = {C:\Users\victo\Zotero\storage\MX3QZUC9\Quiñonero-Candela - 2009 - Dataset shift in machine learning.pdf}
}

@article{rodriguezExperimentalResultsTheoretical,
  title = {Towards Experimental Results - {{Theoretical}} Proposals},
  author = {Rodríguez, Víctor Jiménez},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\5HI5796L\Rodríguez - Towards experimental results - Theoretical proposa.pdf}
}

@online{ruderOverviewGradientDescent2017,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  date = {2017-06-15},
  eprint = {1609.04747},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1609.04747},
  urldate = {2024-06-04},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\PWL6P3HF\Ruder - 2017 - An overview of gradient descent optimization algor.pdf}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E and Hintont, Geoffrey E and Williams, Ronald J},
  date = {1986},
  langid = {english},
  file = {C:\Users\victo\Zotero\storage\GJR7G9DZ\Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf}
}

@online{schaefferDoubleDescentDemystified2023,
  title = {Double {{Descent Demystified}}: {{Identifying}}, {{Interpreting}} \& {{Ablating}} the {{Sources}} of a {{Deep Learning Puzzle}}},
  shorttitle = {Double {{Descent Demystified}}},
  author = {Schaeffer, Rylan and Khona, Mikail and Robertson, Zachary and Boopathy, Akhilan and Pistunova, Kateryna and Rocks, Jason W. and Fiete, Ila Rani and Koyejo, Oluwasanmi},
  date = {2023-03-24},
  eprint = {2303.14151},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2303.14151},
  urldate = {2024-02-28},
  abstract = {Double descent is a surprising phenomenon in machine learning, in which as the number of model parameters grows relative to the number of data, test error drops as models grow ever larger into the highly overparameterized (data undersampled) regime. This drop in test error flies against classical learning theory on overfitting and has arguably underpinned the success of large models in machine learning. This non-monotonic behavior of test loss depends on the number of data, the dimensionality of the data and the number of model parameters. Here, we briefly describe double descent, then provide an explanation of why double descent occurs in an informal and approachable manner, requiring only familiarity with linear algebra and introductory probability. We provide visual intuition using polynomial regression, then mathematically analyze double descent with ordinary linear regression and identify three interpretable factors that, when simultaneously all present, together create double descent. We demonstrate that double descent occurs on real data when using ordinary linear regression, then demonstrate that double descent does not occur when any of the three factors are ablated. We use this understanding to shed light on recent observations in nonlinear models concerning superposition and double descent. Code is publicly available.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\L4LY88BC\Schaeffer et al. - 2023 - Double Descent Demystified Identifying, Interpret.pdf}
}

@article{schmidhuberLearningForgetContinual2000,
  title = {Learning to {{Forget}}: {{Continual Prediction}} with {{LSTM}}},
  author = {Schmidhuber, Jiirgen and Cummins, Fred},
  date = {2000},
  abstract = {Long Short-Term Memory (LSTM,[5])can solve many tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams without explicitly marked sequence ends. Without resets, the internal state values may grow indefinitely and eventually cause the network to break down. Our remedy is an adaptive “forget gate” that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review an illustrative benchmark problem on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve a continual version of that problem. LSTM with forget gates, however, easily solves it in an elegant way.},
  langid = {english},
  keywords = {LSTM,notion},
  file = {C:\Users\victo\Zotero\storage\NUK2QPS9\Schmidhuber i Cummins - Learning to Forget Continual Prediction with LSTM.pdf}
}

@online{schmidtAdversariallyRobustGeneralization2018,
  title = {Adversarially {{Robust Generalization Requires More Data}}},
  author = {Schmidt, Ludwig and Santurkar, Shibani and Tsipras, Dimitris and Talwar, Kunal and Mądry, Aleksander},
  date = {2018-05-02},
  eprint = {1804.11285},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.11285},
  urldate = {2023-09-15},
  abstract = {Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classifiers with high “standard” accuracy to produce an incorrect prediction with high confidence. To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model, the sample complexity of robust learning can be significantly larger than that of “standard” learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classification datasets and show that a similar gap exists here as well. We postulate that the difficulty of training robust classifiers stems, at least partially, from this inherently larger sample complexity.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\JYT33FK5\Schmidt et al. - 2018 - Adversarially Robust Generalization Requires More .pdf}
}

@article{shafahiAdversarialTrainingFree2019,
  title = {Adversarial Training for Free!},
  author = {Shafahi, Ali and Najibi, Mahyar and Ghiasi, Mohammad Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S and Taylor, Gavin and Goldstein, Tom},
  date = {2019},
  abstract = {Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our “free” adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classification task that maintains 40\% accuracy against PGD attacks.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\BNBNIEF3\Shafahi et al. - Adversarial training for free!.pdf}
}

@inproceedings{BMVC2016_87,
        	title={Wide Residual Networks},
        	author={Sergey Zagoruyko and Nikos Komodakis},
        	year={2016},
        	month={September},
        	pages={87.1-87.12},
        	articleno={87},
        	numpages={12},
        	booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
        	publisher={BMVA Press},
        	editor={Richard C. Wilson, Edwin R. Hancock and William A. P. Smith},
        	doi={10.5244/C.30.87},
        	isbn={1-901725-59-6},
        	url={https://dx.doi.org/10.5244/C.30.87}
  }

@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={Proceedings of the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing},
  year={2019},
  organization={Association for Computational Linguistics},
  url={https://arxiv.org/abs/1910.01108}
}

@inproceedings{maas2011learning,
  title={Learning Word Vectors for Sentiment Analysis},
  author={Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
  booktitle={Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  year={2011},
  pages={142--150},
  url={https://aclanthology.org/P11-1015}
}


@inproceedings{resnet50,
  author       = {Kaiming He and
                  Xiangyu Zhang and
                  Shaoqing Ren and
                  Jian Sun},
  title        = {Deep Residual Learning for Image Recognition},
  booktitle    = {Computer Vision and Pattern Recognition},
  pages        = {770--778},
  publisher    = {{IEEE} Computer Society},
  year         = {2016},
}

@misc{engstrom2019adversarial,
      title={Adversarial Robustness as a Prior for Learned Representations}, 
      author={Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Brandon Tran and Aleksander Madry},
      year={2019},
      eprint={1906.00945},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{AthalyeC018,
  author       = {Anish Athalye and
                  Nicholas Carlini and
                  David A. Wagner},
  title        = {Obfuscated Gradients Give a False Sense of Security: Circumventing
                  Defenses to Adversarial Examples},
  booktitle    = {International Conference on Machine Learning},
  volume       = {80},
  pages        = {274--283},
  publisher    = {{PMLR}},
  year         = {2018},
}

@inproceedings{WongRK20,
  author       = {Eric Wong and
                  Leslie Rice and
                  J. Zico Kolter},
  title        = {Fast is better than free: Revisiting adversarial training},
  booktitle    = {International Conference on Learning Representations},
  year         = {2020},
}


@inproceedings{Addepalli2022ScalingAT,
  title={Scaling Adversarial Training to Large Perturbation Bounds},
  author={Sravanti Addepalli and Samyak Jain and Gaurang Sriramanan and R. Venkatesh Babu},
  booktitle={European Conference on Computer Vision},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252968354}
}

@misc{wang2023betterdiffusionmodelsimprove,
      title={Better Diffusion Models Further Improve Adversarial Training}, 
      author={Zekai Wang and Tianyu Pang and Chao Du and Min Lin and Weiwei Liu and Shuicheng Yan},
      year={2023},
      eprint={2302.04638},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2302.04638}, 
}

@online{shenWassersteinDistanceGuided2018,
  title = {Wasserstein {{Distance Guided Representation Learning}} for {{Domain Adaptation}}},
  author = {Shen, Jian and Qu, Yanru and Zhang, Weinan and Yu, Yong},
  date = {2018-03-09},
  eprint = {1707.01217},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1707.01217},
  urldate = {2023-09-18},
  abstract = {Domain adaptation aims at generalizing a high-performance learner on a target domain via utilizing the knowledge distilled from a source domain which has a different but related data distribution. One solution to domain adaptation is to learn domain invariant feature representations while the learned representations should also be discriminative in prediction. To learn such representations, domain adaptation frameworks usually include a domain invariant representation learning approach to measure and reduce the domain discrepancy, as well as a discriminator for classification. Inspired by Wasserstein GAN, in this paper we propose a novel approach to learn domain invariant feature representations, namely Wasserstein Distance Guided Representation Learning (WDGRL). WDGRL utilizes a neural network, denoted by the domain critic, to estimate empirical Wasserstein distance between the source and target samples and optimizes the feature extractor network to minimize the estimated Wasserstein distance in an adversarial manner. The theoretical advantages of Wasserstein distance for domain adaptation lie in its gradient property and promising generalization bound. Empirical studies on common sentiment and image classification adaptation datasets demonstrate that our proposed WDGRL outperforms the state-of-the-art domain invariant representation learning approaches.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {domain adaptation,notion},
  file = {C:\Users\victo\Zotero\storage\37AF7234\Shen et al. - 2018 - Wasserstein Distance Guided Representation Learnin.pdf}
}

@online{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015-04-10},
  eprint = {1409.1556},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1409.1556},
  urldate = {2024-05-06},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{stutzRelatingAdversariallyRobust2021,
  title = {Relating {{Adversarially Robust Generalization}} to {{Flat Minima}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
  date = {2021-10},
  pages = {7787--7797},
  publisher = {IEEE},
  location = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.00771},
  url = {https://ieeexplore.ieee.org/document/9709957/},
  urldate = {2023-09-17},
  abstract = {Adversarial training (AT) has become the de-facto standard to obtain models robust against adversarial examples. However, AT exhibits severe robust overfitting: crossentropy loss on adversarial examples, so-called robust loss, decreases continuously on training examples, while eventually increasing on test examples. In practice, this leads to poor robust generalization, i.e., adversarial robustness does not generalize well to new examples. In this paper, we study the relationship between robust generalization and flatness of the robust loss landscape in weight space, i.e., whether robust loss changes significantly when perturbing weights. To this end, we propose average- and worst-case metrics to measure flatness in the robust loss landscape and show a correlation between good robust generalization and flatness. For example, throughout training, flatness reduces significantly during overfitting such that early stopping effectively finds flatter minima in the robust loss landscape. Similarly, AT variants achieving higher adversarial robustness also correspond to flatter minima. This holds for many popular choices, e.g., AT-AWP, TRADES, MART, AT with self-supervision or additional unlabeled examples, as well as simple regularization techniques, e.g., AutoAugment, weight decay or label noise. For fair comparison across these approaches, our flatness measures are specifically designed to be scale-invariant and we conduct extensive experiments to validate our findings.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\VX2HCUL4\Stutz et al. - 2021 - Relating Adversarially Robust Generalization to Fl.pdf}
}

@incollection{suRobustnessCostAccuracy2018,
  title = {Is {{Robustness}} the {{Cost}} of {{Accuracy}}? – {{A Comprehensive Study}} on the {{Robustness}} of 18 {{Deep Image Classification Models}}},
  shorttitle = {Is {{Robustness}} the {{Cost}} of {{Accuracy}}?},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Su, Dong and Zhang, Huan and Chen, Hongge and Yi, Jinfeng and Chen, Pin-Yu and Gao, Yupeng},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  volume = {11216},
  pages = {644--661},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-01258-8_39},
  url = {https://link.springer.com/10.1007/978-3-030-01258-8_39},
  urldate = {2024-04-30},
  isbn = {978-3-030-01257-1 978-3-030-01258-8},
  langid = {english},
  file = {C:\Users\victo\Zotero\storage\KR7U5TGF\Su et al. - 2018 - Is Robustness the Cost of Accuracy – A Comprehens.pdf}
}

@online{szegedyIntriguingPropertiesNeural2014,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  date = {2014-02-19},
  eprint = {1312.6199},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1312.6199},
  urldate = {2024-05-01},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {C:\Users\victo\Zotero\storage\USVHTGRM\Szegedy et al. - 2014 - Intriguing properties of neural networks.pdf}
}

@inproceedings{torralbaUnbiasedLookDataset2011,
  title = {Unbiased Look at Dataset Bias},
  booktitle = {{{CVPR}} 2011},
  author = {Torralba, Antonio and Efros, Alexei A.},
  date = {2011-06},
  pages = {1521--1528},
  publisher = {IEEE},
  location = {Colorado Springs, CO, USA},
  doi = {10.1109/CVPR.2011.5995347},
  url = {http://ieeexplore.ieee.org/document/5995347/},
  urldate = {2024-05-06},
  abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
  eventtitle = {2011 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4577-0394-2},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\8C3PFPBB\Torralba i Efros - 2011 - Unbiased look at dataset bias.pdf}
}

@online{tsiprasRobustnessMayBe2019,
  title = {Robustness {{May Be}} at {{Odds}} with {{Accuracy}}},
  author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  date = {2019-09-09},
  eprint = {1805.12152},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.12152},
  urldate = {2023-09-15},
  abstract = {We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion,robustness vs accuracy},
  file = {C:\Users\victo\Zotero\storage\NDZYMQZA\Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf}
}

@article{voulodimosDeepLearningComputer2018,
  title = {Deep {{Learning}} for {{Computer Vision}}: {{A Brief Review}}},
  shorttitle = {Deep {{Learning}} for {{Computer Vision}}},
  author = {Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
  date = {2018},
  journaltitle = {Computational Intelligence and Neuroscience},
  shortjournal = {Computational Intelligence and Neuroscience},
  volume = {2018},
  pages = {1--13},
  issn = {1687-5265, 1687-5273},
  doi = {10.1155/2018/7068349},
  url = {https://www.hindawi.com/journals/cin/2018/7068349/},
  urldate = {2024-04-22},
  abstract = {Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.},
  langid = {english},
  file = {C:\Users\victo\Zotero\storage\ZMC7QF3C\Voulodimos et al. - 2018 - Deep Learning for Computer Vision A Brief Review.pdf}
}

@online{wangBetterDiffusionModels2023,
  title = {Better {{Diffusion Models Further Improve Adversarial Training}}},
  author = {Wang, Zekai and Pang, Tianyu and Du, Chao and Lin, Min and Liu, Weiwei and Yan, Shuicheng},
  date = {2023-06-01},
  eprint = {2302.04638},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.04638},
  urldate = {2023-10-19},
  abstract = {It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model (Karras et al., 2022) which has higher efficiency (∼ 20 sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the ℓ∞norm threat model with ϵ = 8/255, our models achieve 70.69\% and 42.67\% robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by +4.58\% and +8.03\%. Under the ℓ2-norm threat model with ϵ = 128/255, our models achieve 84.86\% on CIFAR-10 (+4.44\%). These results also beat previous works that use external data. We also provide compelling results on the SVHN and TinyImageNet datasets. Our code is at https://github.com/wzekai99/DM-Improves-AT.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {DDPM,notion},
  file = {C:\Users\victo\Zotero\storage\4MB39JXE\Wang et al. - 2023 - Better Diffusion Models Further Improve Adversaria.pdf}
}

@online{wangGeneralizingUnseenDomains2022,
  title = {Generalizing to {{Unseen Domains}}: {{A Survey}} on {{Domain Generalization}}},
  shorttitle = {Generalizing to {{Unseen Domains}}},
  author = {Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Qin, Tao and Lu, Wang and Chen, Yiqiang and Zeng, Wenjun and Yu, Philip S.},
  date = {2022-05-23},
  eprint = {2103.03097},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.03097},
  urldate = {2024-05-06},
  abstract = {Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {C:\Users\victo\Zotero\storage\EK88CDFZ\Wang et al. - 2022 - Generalizing to Unseen Domains A Survey on Domain.pdf}
}

@article{wangGeometricalApproachEvaluate2023,
  title = {A {{Geometrical Approach}} to {{Evaluate}} the {{Adversarial Robustness}} of {{Deep Neural Networks}}},
  author = {Wang, Yang and Dong, Bo and Xu, Ke and Piao, Haiyin and Ding, Yufei and Yin, Baocai and Yang, Xin},
  date = {2023-10-31},
  journaltitle = {ACM Transactions on Multimedia Computing, Communications, and Applications},
  shortjournal = {ACM Trans. Multimedia Comput. Commun. Appl.},
  volume = {19},
  eprint = {2310.06468},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--17},
  issn = {1551-6857, 1551-6865},
  doi = {10.1145/3587936},
  url = {http://arxiv.org/abs/2310.06468},
  urldate = {2024-03-27},
  abstract = {Deep Neural Networks (DNNs) are widely used for computer vision tasks. However, it has been shown that deep models are vulnerable to adversarial attacks, i.e., their performances drop when imperceptible perturbations are made to the original inputs, which may further degrade the following visual tasks or introduce new problems such as data and privacy security. Hence, metrics for evaluating the robustness of deep models against adversarial attacks are desired. However, previous metrics are mainly proposed for evaluating the adversarial robustness of shallow networks on the small-scale datasets. Although the Cross Lipschitz Extreme Value for nEtwork Robustness (CLEVER) metric has been proposed for large-scale datasets (e.g., the ImageNet dataset), it is computationally expensive and its performance relies on a tractable number of samples. In this paper, we propose the Adversarial Converging Time Score (ACTS), an attack-dependent metric that quantifies the adversarial robustness of a DNN on a specific input. Our key observation is that local neighborhoods on a DNN’s output surface would have different shapes given different inputs. Hence, given different inputs, it requires different time for converging to an adversarial sample. Based on this geometry meaning, ACTS measures the converging time as an adversarial robustness metric. We validate the effectiveness and generalization of the proposed ACTS metric against different adversarial attacks on the large-scale ImageNet dataset using state-of-the-art deep networks. Extensive experiments show that our ACTS metric is an efficient and effective adversarial metric over the previous CLEVER metric. CCS Concepts: • Computing methodologies → Computer vision; Adversarial learning.},
  issue = {5s},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\victo\Zotero\storage\97K3HF9E\Wang et al. - 2023 - A Geometrical Approach to Evaluate the Adversarial.pdf}
}

@article{wangImageQualityAssessment2004,
  title = {Image {{Quality Assessment}}: {{From Error Visibility}} to {{Structural Similarity}}},
  shorttitle = {Image {{Quality Assessment}}},
  author = {Wang, Z. and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  date = {2004-04},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1057-7149},
  doi = {10.1109/TIP.2003.819861},
  url = {http://ieeexplore.ieee.org/document/1284395/},
  urldate = {2024-05-07},
  langid = {english},
  file = {C:\Users\victo\Zotero\storage\325VE93U\Wang et al. - 2004 - Image Quality Assessment From Error Visibility to.pdf}
}

@article{wangINFOBERTIMPROVINGROBUSTNESS2021,
  title = {{{INFOBERT}}: {{IMPROVING ROBUSTNESS OF LANGUAGE MODELS FROM AN INFORMATION THEORETIC PERSPECTIVE}}},
  author = {Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
  date = {2021},
  abstract = {Large-scale pre-trained language models such as BERT and RoBERTa have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing the threats of textual adversarial attacks. We aim to address this problem from an information-theoretic perspective, and propose InfoBERT, a novel learning framework for robust fine-tuning of pre-trained language models. InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) an Anchored Feature regularizer, which increases the mutual information between local stable features and global features. We provide a principled way to theoretically analyze and improve the robustness of language models in both standard and adversarial training. Extensive experiments demonstrate that InfoBERT achieves stateof-the-art robust accuracy over several adversarial datasets on Natural Language Inference (NLI) and Question Answering (QA) tasks. Our code is available at https://github.com/AI-secure/InfoBERT.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\HYHAFDBY\Wang et al. - 2021 - INFOBERT IMPROVING ROBUSTNESS OF LANGUAGE MODELS .pdf}
}

@online{wangMetaFineTuningNeural2020,
  title = {Meta {{Fine-Tuning Neural Language Models}} for {{Multi-Domain Text Mining}}},
  author = {Wang, Chengyu and Qiu, Minghui and Huang, Jun and He, Xiaofeng},
  date = {2020-09-16},
  eprint = {2003.13003},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.13003},
  urldate = {2023-09-18},
  abstract = {Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), serving as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domaininvariant representations by optimizing a series of novel domain corruption loss functions. After MFT, the model can be fine-tuned for each domain with better parameter initialization and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {meta fine-tuning,notion},
  file = {C:\Users\victo\Zotero\storage\PIUVB77I\Wang et al. - 2020 - Meta Fine-Tuning Neural Language Models for Multi-.pdf}
}

@online{wengEvaluatingRobustnessNeural2018,
  title = {Evaluating the {{Robustness}} of {{Neural Networks}}: {{An Extreme Value Theory Approach}}},
  shorttitle = {Evaluating the {{Robustness}} of {{Neural Networks}}},
  author = {Weng, Tsui-Wei and Zhang, Huan and Chen, Pin-Yu and Yi, Jinfeng and Su, Dong and Gao, Yupeng and Hsieh, Cho-Jui and Daniel, Luca},
  date = {2018-01-31},
  eprint = {1801.10578},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1801.10578},
  urldate = {2024-03-27},
  abstract = {The robustness of neural networks to adversarial examples has received great attention due to security implications. Despite various attack approaches to crafting visually imperceptible adversarial examples, little has been developed towards a comprehensive measure of robustness. In this paper, we provide a theoretical justification for converting robustness analysis into a local Lipschitz constant estimation problem, and propose to use the Extreme Value Theory for efficient evaluation. Our analysis yields a novel robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness. The proposed CLEVER score is attack-agnostic and computationally feasible for large neural networks. Experimental results on various networks, including ResNet, Inceptionv3 and MobileNet, show that (i) CLEVER is aligned with the robustness indication measured by the 2 and ∞ norms of adversarial examples from powerful attacks, and (ii) defended networks using defensive distillation or bounded ReLU indeed achieve better CLEVER scores. To the best of our knowledge, CLEVER is the first attack-independent robustness metric that can be applied to any neural network classifier.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\DMBY632F\Weng et al. - 2018 - Evaluating the Robustness of Neural Networks An E.pdf}
}

@article{wulffConvexOptimizationBuilding,
  title = {Convex {{Optimization}} as a {{Building Block}} for {{Difficult Problems}} in {{Machine Learning}}},
  author = {Wulff, Sharon and family=Zürich, given=ETH, given-i=ETH},
  langid = {english},
  file = {C:\Users\victo\Zotero\storage\B8EDVB8P\Wulff i Zürich - Convex Optimization as a Building Block for Diffic.pdf}
}

@online{xiaoGeneratingAdversarialExamples2019,
  title = {Generating {{Adversarial Examples}} with {{Adversarial Networks}}},
  author = {Xiao, Chaowei and Li, Bo and Zhu, Jun-Yan and He, Warren and Liu, Mingyan and Song, Dawn},
  date = {2019-02-14},
  eprint = {1801.02610},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1801.02610},
  urldate = {2023-09-14},
  abstract = {Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs. Such adversarial examples can mislead DNNs to produce adversary-selected results. Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses. We apply AdvGAN in both semi-whitebox and black-box attack settings. In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks. In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly. Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks. Our attack has placed the first with 92.76\% accuracy on a public MNIST black-box attack challenge.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {GANs,notion},
  file = {C:\Users\victo\Zotero\storage\W6YQ68U7\Xiao et al. - 2019 - Generating Adversarial Examples with Adversarial N.pdf}
}

@online{yaoImprovingOutofDistributionRobustness2022,
  title = {Improving {{Out-of-Distribution Robustness}} via {{Selective Augmentation}}},
  author = {Yao, Huaxiu and Wang, Yu and Li, Sai and Zhang, Linjun and Liang, Weixin and Zou, James and Finn, Chelsea},
  date = {2022-06-19},
  eprint = {2201.00299},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.00299},
  urldate = {2023-10-20},
  abstract = {Machine learning algorithms typically assume that training and test examples are drawn from the same distribution. However, distribution shift is a common problem in real-world applications and can cause models to perform dramatically worse at test time. In this paper, we specifically consider the problems of subpopulation shifts (e.g., imbalanced data) and domain shifts. While prior works often seek to explicitly regularize internal representations or predictors of the model to be domain invariant, we instead aim to learn invariant predictors without restricting the model's internal representations or predictors. This leads to a simple mixup-based technique which learns invariant predictors via selective augmentation called LISA. LISA selectively interpolates samples either with the same labels but different domains or with the same domain but different labels. Empirically, we study the effectiveness of LISA on nine benchmarks ranging from subpopulation shifts to domain shifts, and we find that LISA consistently outperforms other state-of-the-art methods and leads to more invariant predictors. We further analyze a linear setting and theoretically show how LISA leads to a smaller worst-group error.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {LISA},
  file = {C:\Users\victo\Zotero\storage\P74LQCD7\Yao et al. - 2022 - Improving Out-of-Distribution Robustness via Selec.pdf}
}

@online{yunCutMixRegularizationStrategy2019,
  title = {{{CutMix}}: {{Regularization Strategy}} to {{Train Strong Classifiers}} with {{Localizable Features}}},
  shorttitle = {{{CutMix}}},
  author = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  date = {2019-08-07},
  eprint = {1905.04899},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1905.04899},
  urldate = {2024-02-20},
  abstract = {Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\QWFLL3BG\Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf}
}

@online{yuPACSDatasetPhysical2022,
  title = {{{PACS}}: {{A Dataset}} for {{Physical Audiovisual CommonSense Reasoning}}},
  shorttitle = {{{PACS}}},
  author = {Yu, Samuel and Wu, Peter and Liang, Paul Pu and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  date = {2022-08-01},
  eprint = {2203.11130},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.11130},
  urldate = {2024-05-07},
  abstract = {In order for AI to be safely deployed in real-world scenarios such as hospitals, schools, and the workplace, it must be able to robustly reason about the physical world. Fundamental to this reasoning is physical common sense: understanding the physical properties and affordances of available objects, how they can be manipulated, and how they interact with other objects. Physical commonsense reasoning is fundamentally a multi-sensory task, since physical properties are manifested through multiple modalities - two of them being vision and acoustics. Our paper takes a step towards real-world physical commonsense reasoning by contributing PACS: the first audiovisual benchmark annotated for physical commonsense attributes. PACS contains 13,400 question-answer pairs, involving 1,377 unique physical commonsense questions and 1,526 videos. Our dataset provides new opportunities to advance the research field of physical reasoning by bringing audio as a core component of this multimodal problem. Using PACS, we evaluate multiple state-of-the-art models on our new challenging task. While some models show promising results (70\% accuracy), they all fall short of human performance (95\% accuracy). We conclude the paper by demonstrating the importance of multimodal reasoning and providing possible avenues for future research.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  file = {C:\Users\victo\Zotero\storage\YRFH4RBW\Yu et al. - 2022 - PACS A Dataset for Physical Audiovisual CommonSen.pdf}
}

@inproceedings{zengOpenAttackOpensourceTextual2021,
  title = {{{OpenAttack}}: {{An Open-source Textual Adversarial Attack Toolkit}}},
  shorttitle = {{{OpenAttack}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Zeng, Guoyang and Qi, Fanchao and Zhou, Qianrui and Zhang, Tingji and Ma, Zixian and Hou, Bairu and Zang, Yuan and Liu, Zhiyuan and Sun, Maosong},
  date = {2021},
  eprint = {2009.09191},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {363--371},
  doi = {10.18653/v1/2021.acl-demo.43},
  url = {http://arxiv.org/abs/2009.09191},
  urldate = {2023-09-18},
  abstract = {Textual adversarial attacking has received wide and increasing attention in recent years. Various attack models have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and fair comparison of attack models. In this paper, we present an opensource textual adversarial attack toolkit named OpenAttack to solve these issues. Compared with existing other textual adversarial attack toolkits, OpenAttack has its unique strengths in support for all attack types, multilinguality, and parallel processing. Currently, OpenAttack includes 15 typical attack models that cover all attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. OpenAttack has broad uses including comparing and evaluating attack models, measuring robustness of a model, assisting in developing new attack models, and adversarial training. Source code and documentation can be obtained at https://github.com/thunlp/ OpenAttack.},
  langid = {english},
  keywords = {adversarial attack,notion},
  file = {C:\Users\victo\Zotero\storage\4E2NPLFM\Zeng et al. - 2021 - OpenAttack An Open-source Textual Adversarial Att.pdf}
}

@online{zhangMixupEmpiricalRisk2018,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle = {Mixup},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
  date = {2018-04-27},
  eprint = {1710.09412},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1710.09412},
  urldate = {2024-05-06},
  abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\victo\Zotero\storage\KQ5ZG8N2\Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf}
}

@online{zhangTheoreticallyPrincipledTradeoff2019,
  title = {Theoretically {{Principled Trade-off}} between {{Robustness}} and {{Accuracy}}},
  author = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric P. and Ghaoui, Laurent El and Jordan, Michael I.},
  date = {2019-06-24},
  eprint = {1901.08573},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1901.08573},
  urldate = {2023-09-15},
  abstract = {We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of \textasciitilde 2,000 submissions, surpassing the runner-up approach by 11.41\% in terms of mean 2 perturbation distance.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion,robustness vs accuracy},
  file = {C:\Users\victo\Zotero\storage\PBMRK2LA\Zhang et al. - 2019 - Theoretically Principled Trade-off between Robustn.pdf}
}

@article{zhouDomainGeneralizationSurvey2022,
  title = {Domain {{Generalization}}: {{A Survey}}},
  shorttitle = {Domain {{Generalization}}},
  author = {Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
  date = {2022},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  eprint = {2103.02503},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--20},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2022.3195549},
  url = {http://arxiv.org/abs/2103.02503},
  urldate = {2024-05-06},
  abstract = {Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other relevant fields like domain adaptation and transfer learning. Then, we conduct a thorough review into existing methods and theories. Finally, we conclude this survey with insights and discussions on future research directions.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {C:\Users\victo\Zotero\storage\AWW7T3ZP\Zhou et al. - 2022 - Domain Generalization A Survey.pdf}
}

@online{zhouLIMITBERTLinguisticInformed2020,
  title = {{{LIMIT-BERT}} : {{Linguistic Informed Multi-Task BERT}}},
  shorttitle = {{{LIMIT-BERT}}},
  author = {Zhou, Junru and Zhang, Zhuosheng and Zhao, Hai and Zhang, Shuailiang},
  date = {2020-10-05},
  eprint = {1910.14296},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1910.14296},
  urldate = {2023-09-18},
  abstract = {In this paper, we present Linguistics Informed Multi-Task BERT (LIMIT-BERT) for learning language representations across multiple linguistics tasks by Multi-Task Learning. LIMITBERT includes five key linguistics tasks: PartOf-Speech (POS) tags, constituent and dependency syntactic parsing, span and dependency semantic role labeling (SRL). Different from recent Multi-Task Deep Neural Networks (MT-DNN), our LIMIT-BERT is fully linguistics motivated and thus is capable of adopting an improved masked training objective according to syntactic and semantic constituents. Besides, LIMIT-BERT takes a semisupervised learning strategy to offer the same large amount of linguistics task data as that for the language model training. As a result, LIMIT-BERT not only improves linguistics tasks performance, but also benefits from a regularization effect and linguistics information that leads to more general representations to help adapt to new tasks and domains. LIMIT-BERT outperforms the strong baseline Whole Word Masking BERT on both dependency and constituent syntactic/semantic parsing, GLUE benchmark, and SNLI task. Our practice on the proposed LIMIT-BERT also enables us to release a well pre-trained model for multi-purpose of natural language processing tasks once for all.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {multi-task learning,notion},
  file = {C:\Users\victo\Zotero\storage\2BEYZ7XT\Zhou et al. - 2020 - LIMIT-BERT  Linguistic Informed Multi-Task BERT.pdf}
}

@article{zhuFREELBENHANCEDADVERSARIAL2020,
  title = {{{FREELB}}: {{ENHANCED ADVERSARIAL TRAINING FOR NATURAL LANGUAGE UNDERSTANDING}}},
  author = {Zhu, Chen and Cheng, Yu and Gan, Zhe and Sun, Siqi and Goldstein, Tom and Liu, Jingjing},
  date = {2020},
  abstract = {Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm, FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of BERT-base model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44\% and 67.75\% on ARC-Easy and ARC-Challenge. Experiments on CommonsenseQA benchmark further demonstrate that FreeLB can be generalized and boost the performance of RoBERTa-large model on other tasks as well.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\victo\Zotero\storage\VXYY5BDR\Zhu et al. - 2020 - FREELB ENHANCED ADVERSARIAL TRAINING FOR NATURAL .pdf}
}

@misc{faiss,
  title = {FAISS: A library for efficient similarity search and clustering of dense vectors},
  author = {{Facebook AI Research}},
  year = {2017},
  howpublished = {\url{https://github.com/facebookresearch/faiss}},
  note = {Accessed: 2024-09-15}
}